ncu is available, will collect DRAM metrics

==========================================
Running copy kernel with different sizes
==========================================

Comparing all kernels: baseline, loop_unroll (times=4), vectorize, vectorize_unroll
Fixed parameters: block_dim=256, loop_unroll_times=4

========================================
Testing with 2^10 = 1024 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^10 = 1024 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.119808 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8041 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 555.16 ms
Copy successful
==PROF== Disconnected from process 8041
[8041] copy@127.0.0.1
  copy_baseline(float *, float *, int) (4, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte    7.04    7.04    7.04
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.16    4.16    4.16
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^10 = 1024 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.116736 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8066 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 599.529 ms
Copy successful
==PROF== Disconnected from process 8066
[8066] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (1, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte    7.81    7.81    7.81
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    6.34    6.34    6.34
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^10 = 1024 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.116736 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8091 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 567.876 ms
Copy successful
==PROF== Disconnected from process 8091
[8091] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (1, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte    7.04    7.04    7.04
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.54    4.54    4.54
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^10 = 1024 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.135168 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8116 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 587.525 ms
Copy successful
==PROF== Disconnected from process 8116
[8116] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (1, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte    7.30    7.30    7.30
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.61    4.61    4.61
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^12 = 4096 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^12 = 4096 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.12288 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8141 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 595.537 ms
Copy successful
==PROF== Disconnected from process 8141
[8141] copy@127.0.0.1
  copy_baseline(float *, float *, int) (16, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   19.46   19.46   19.46
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.32    4.32    4.32
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^12 = 4096 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.124928 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8166 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 671.11 ms
Copy successful
==PROF== Disconnected from process 8166
[8166] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (4, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   20.22   20.22   20.22
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    5.66    5.66    5.66
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^12 = 4096 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.144384 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8191 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 595.401 ms
Copy successful
==PROF== Disconnected from process 8191
[8191] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (4, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   19.33   19.33   19.33
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.19    4.19    4.19
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^12 = 4096 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.114688 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8216 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 567.116 ms
Copy successful
==PROF== Disconnected from process 8216
[8216] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (1, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   19.58   19.58   19.58
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    6.43    6.43    6.43
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^14 = 16384 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^14 = 16384 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.118784 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8241 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 605.789 ms
Copy successful
==PROF== Disconnected from process 8241
[8241] copy@127.0.0.1
  copy_baseline(float *, float *, int) (64, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   68.48   68.48   68.48
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.32    4.32    4.32
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^14 = 16384 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.120832 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8266 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 591.43 ms
Copy successful
==PROF== Disconnected from process 8266
[8266] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (16, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   69.25   69.25   69.25
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    6.14    6.14    6.14
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^14 = 16384 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.119808 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8291 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 558.228 ms
Copy successful
==PROF== Disconnected from process 8291
[8291] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (16, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   68.48   68.48   68.48
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.38    4.38    4.38
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^14 = 16384 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.141312 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8316 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 575.446 ms
Copy successful
==PROF== Disconnected from process 8316
[8316] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (2, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte   68.74   68.74   68.74
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    9.06    9.06    9.06
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^16 = 65536 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^16 = 65536 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.114624 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8341 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 599.333 ms
Copy successful
==PROF== Disconnected from process 8341
[8341] copy@127.0.0.1
  copy_baseline(float *, float *, int) (256, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte  265.09  265.09  265.09
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.38    4.38    4.38
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^16 = 65536 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.120608 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8366 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 595.331 ms
Copy successful
==PROF== Disconnected from process 8366
[8366] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (64, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte  265.86  265.86  265.86
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    6.05    6.05    6.05
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^16 = 65536 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.107104 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8391 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 557.942 ms
Copy successful
==PROF== Disconnected from process 8391
[8391] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (64, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte  265.22  265.22  265.22
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    4.32    4.32    4.32
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^16 = 65536 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.114144 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8416 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 607.494 ms
Copy successful
==PROF== Disconnected from process 8416
[8416] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (8, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Kbyte  265.34  265.34  265.34
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    8.70    8.70    8.70
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^18 = 262144 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^18 = 262144 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.06 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8441 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 622.972 ms
Copy successful
==PROF== Disconnected from process 8441
[8441] copy@127.0.0.1
  copy_baseline(float *, float *, int) (1024, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    1.05    1.05    1.05
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    5.63    5.63    5.63
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^18 = 262144 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.049536 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8466 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 571.068 ms
Copy successful
==PROF== Disconnected from process 8466
[8466] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (256, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    1.05    1.05    1.05
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    6.85    6.85    6.85
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^18 = 262144 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.062208 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8491 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 602.351 ms
Copy successful
==PROF== Disconnected from process 8491
[8491] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (256, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    1.05    1.05    1.05
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    5.18    5.18    5.18
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^18 = 262144 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.066368 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8516 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 619.054 ms
Copy successful
==PROF== Disconnected from process 8516
[8516] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (32, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    1.05    1.05    1.05
    dram__bytes_write.sum         byte    0.00    0.00    0.00
    gpu__time_duration.sum          us    9.25    9.25    9.25
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^20 = 1048576 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^20 = 1048576 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.060032 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8541 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 602.044 ms
Copy successful
==PROF== Disconnected from process 8541
[8541] copy@127.0.0.1
  copy_baseline(float *, float *, int) (4096, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    4.20    4.20    4.20
    dram__bytes_write.sum         byte  128.00  128.00  128.00
    gpu__time_duration.sum          us    9.47    9.47    9.47
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^20 = 1048576 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.062336 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8566 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 553.161 ms
Copy successful
==PROF== Disconnected from process 8566
[8566] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (1024, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    4.20    4.20    4.20
    dram__bytes_write.sum         byte  128.00  128.00  128.00
    gpu__time_duration.sum          us   10.40   10.40   10.40
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^20 = 1048576 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.05552 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8591 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 574.079 ms
Copy successful
==PROF== Disconnected from process 8591
[8591] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (1024, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    4.20    4.20    4.20
    dram__bytes_write.sum         byte  128.00  128.00  128.00
    gpu__time_duration.sum          us    8.19    8.19    8.19
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^20 = 1048576 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.066304 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8616 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 594.858 ms
Copy successful
==PROF== Disconnected from process 8616
[8616] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (128, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte    4.20    4.20    4.20
    dram__bytes_write.sum         byte  128.00  128.00  128.00
    gpu__time_duration.sum          us   10.75   10.75   10.75
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^22 = 4194304 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^22 = 4194304 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.07104 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8641 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 598.716 ms
Copy successful
==PROF== Disconnected from process 8641
[8641] copy@127.0.0.1
  copy_baseline(float *, float *, int) (16384, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   16.78   16.78   16.78
    dram__bytes_write.sum        Mbyte    4.33    4.33    4.33
    gpu__time_duration.sum          us   25.44   25.44   25.44
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^22 = 4194304 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.07408 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8666 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 597.905 ms
Copy successful
==PROF== Disconnected from process 8666
[8666] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (4096, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   16.78   16.78   16.78
    dram__bytes_write.sum        Mbyte    4.32    4.32    4.32
    gpu__time_duration.sum          us   24.26   24.26   24.26
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^22 = 4194304 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.074656 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8691 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 595.503 ms
Copy successful
==PROF== Disconnected from process 8691
[8691] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (4096, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   16.78   16.78   16.78
    dram__bytes_write.sum        Mbyte    3.46    3.46    3.46
    gpu__time_duration.sum          us   18.78   18.78   18.78
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^22 = 4194304 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.072512 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8716 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 578.824 ms
Copy successful
==PROF== Disconnected from process 8716
[8716] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (512, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   16.78   16.78   16.78
    dram__bytes_write.sum        Mbyte    3.80    3.80    3.80
    gpu__time_duration.sum          us   19.74   19.74   19.74
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^24 = 16777216 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^24 = 16777216 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.138624 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8741 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 576.273 ms
Copy successful
==PROF== Disconnected from process 8741
[8741] copy@127.0.0.1
  copy_baseline(float *, float *, int) (65536, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   67.11   67.11   67.11
    dram__bytes_write.sum        Mbyte   54.79   54.79   54.79
    gpu__time_duration.sum          us   95.07   95.07   95.07
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^24 = 16777216 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.135488 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8766 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 567.328 ms
Copy successful
==PROF== Disconnected from process 8766
[8766] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (16384, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   67.11   67.11   67.11
    dram__bytes_write.sum        Mbyte   53.49   53.49   53.49
    gpu__time_duration.sum          us   90.08   90.08   90.08
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^24 = 16777216 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.128928 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8791 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 594.994 ms
Copy successful
==PROF== Disconnected from process 8791
[8791] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (16384, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   67.11   67.11   67.11
    dram__bytes_write.sum        Mbyte   50.55   50.55   50.55
    gpu__time_duration.sum          us   73.50   73.50   73.50
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^24 = 16777216 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.14304 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8816 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 572.328 ms
Copy successful
==PROF== Disconnected from process 8816
[8816] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (2048, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte   67.11   67.11   67.11
    dram__bytes_write.sum        Mbyte   51.66   51.66   51.66
    gpu__time_duration.sum          us   76.58   76.58   76.58
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^26 = 67108864 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^26 = 67108864 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 0.3864 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8841 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 612.487 ms
Copy successful
==PROF== Disconnected from process 8841
[8841] copy@127.0.0.1
  copy_baseline(float *, float *, int) (262144, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte  268.44  268.44  268.44
    dram__bytes_write.sum        Mbyte  255.57  255.57  255.57
    gpu__time_duration.sum          us  377.06  377.06  377.06
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^26 = 67108864 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 0.38432 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8866 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 608.5 ms
Copy successful
==PROF== Disconnected from process 8866
[8866] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (65536, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte  268.45  268.45  268.45
    dram__bytes_write.sum        Mbyte  255.58  255.58  255.58
    gpu__time_duration.sum          us  354.59  354.59  354.59
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^26 = 67108864 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 0.363168 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8891 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 590.118 ms
Copy successful
==PROF== Disconnected from process 8891
[8891] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (65536, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte  268.44  268.44  268.44
    dram__bytes_write.sum        Mbyte  252.37  252.37  252.37
    gpu__time_duration.sum          us  305.18  305.18  305.18
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^26 = 67108864 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 0.40016 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8916 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 571.671 ms
Copy successful
==PROF== Disconnected from process 8916
[8916] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (8192, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Mbyte  268.44  268.44  268.44
    dram__bytes_write.sum        Mbyte  253.97  253.97  253.97
    gpu__time_duration.sum          us  320.29  320.29  320.29
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^28 = 268435456 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^28 = 268435456 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 1.43584 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8941 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 600.56 ms
Copy successful
==PROF== Disconnected from process 8941
[8941] copy@127.0.0.1
  copy_baseline(float *, float *, int) (1048576, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Gbyte    1.07    1.07    1.07
    dram__bytes_write.sum        Gbyte    1.06    1.06    1.06
    gpu__time_duration.sum          ms    1.50    1.50    1.50
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^28 = 268435456 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 1.43274 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8966 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_loop_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 630.493 ms
Copy successful
==PROF== Disconnected from process 8966
[8966] copy@127.0.0.1
  copy_loop_unroll(float *, float *, int) (262144, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Gbyte    1.07    1.07    1.07
    dram__bytes_write.sum        Gbyte    1.06    1.06    1.06
    gpu__time_duration.sum          ms    1.44    1.44    1.44
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize kernel...
----------------------------------------
Testing with 2^28 = 268435456 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize
----------------------------------------
Kernel execution time: 1.32134 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 8991 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize": 0%....50%....100% - 1 pass
Kernel execution time: 571.284 ms
Copy successful
==PROF== Disconnected from process 8991
[8991] copy@127.0.0.1
  copy_vectorize(float *, float *, int) (262144, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Gbyte    1.07    1.07    1.07
    dram__bytes_write.sum        Gbyte    1.06    1.06    1.06
    gpu__time_duration.sum          ms    1.25    1.25    1.25
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running vectorize_unroll kernel...
----------------------------------------
Testing with 2^28 = 268435456 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: vectorize_unroll
----------------------------------------
Kernel execution time: 1.35034 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 9016 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_vectorize_unroll": 0%....50%....100% - 1 pass
Kernel execution time: 603.158 ms
Copy successful
==PROF== Disconnected from process 9016
[9016] copy@127.0.0.1
  copy_vectorize_unroll(float *, float *, int) (32768, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Gbyte    1.07    1.07    1.07
    dram__bytes_write.sum        Gbyte    1.06    1.06    1.06
    gpu__time_duration.sum          ms    1.28    1.28    1.28
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


========================================

========================================
Testing with 2^30 = 1073741824 elements
========================================

>>> Running baseline kernel...
----------------------------------------
Testing with 2^30 = 1073741824 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Kernel: baseline
----------------------------------------
Kernel execution time: 5.71264 ms
Copy successful

Collecting DRAM metrics with ncu...

--- ncu Raw Measurement Results ---
==PROF== Connected to process 9041 (/workspace/cuda-learning/cuda-copy/copy)
==PROF== Profiling "copy_baseline": 0%....50%....100% - 1 pass
Kernel execution time: 632.072 ms
Copy successful
==PROF== Disconnected from process 9041
[9041] copy@127.0.0.1
  copy_baseline(float *, float *, int) (4194304, 1, 1)x(256, 1, 1), Device 0, CC 8.0, Invocations 1
    Section: Command line profiler metrics
    ---------------------- ----------- ------- ------- -------
    Metric Name            Metric Unit Minimum Maximum Average
    ---------------------- ----------- ------- ------- -------
    dram__bytes_read.sum         Gbyte    4.30    4.30    4.30
    dram__bytes_write.sum        Gbyte    4.28    4.28    4.28
    gpu__time_duration.sum          ms    6.07    6.07    6.07
    ---------------------- ----------- ------- ------- -------

  Note: The shown averages are calculated as the arithmetic mean of the metric values after the evaluation of the    
  metrics for each individual kernel launch.                                                                         
  If aggregating across varying launch configurations (like shared memory, cache config settings), the arithmetic    
  mean can be misleading and looking at the individual results is recommended instead.                               


>>> Running loop_unroll kernel (times=4)...
----------------------------------------
Testing with 2^30 = 1073741824 elements
----------------------------------------
----------------------------------------
Testing with block_dim = 256 threads per block
Testing with loop_unroll_times = 4
Kernel: loop_unroll
----------------------------------------
Kernel execution time: 5.63478 ms
